@inproceedings{Jacob:2018,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2704--2713},
  year={2018}
}

@article{Wu:2023,
  title={ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats},
  author={Wu, Xiaoxia and Yao, Zhewei and He, Yuxiong},
  journal={arXiv preprint arXiv:2307.09782},
  year={2023}
}

@article{Han:2015,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{Frankle:2019,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{Chen:2022,
  title={A comprehensive survey of neural network compression},
  author={Chen, Yu and Li, Jian and Xiao, Han and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={5436--5454},
  year={2022},
  publisher={IEEE}
}

@article{Nagel:2021,
  title={A white paper on neural network quantization},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  year={2021}
}

@article{Gale:2019,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{Louizos:2018,
  title={Learning sparse neural networks through L0 regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{Dettmers:2024,
  title={SpQR: A sparse-quantized representation for near-lossless LLM weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{Lin:2024,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyao and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2024}
}

@inproceedings{Frantar:2023,
  title={SparseGPT: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@inproceedings{Zhang:2024,
  title={Integer or floating point? New outlooks for low-bit quantization on large language models},
  author={Zhang, Yijia and Zhao, Liangjie and Cao, Sheng and Zhang, Shuang and Wang, Wei and Cao, Ting and Yang, Zhen and Li, Jing},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@article{Xu:2024,
  title={OneBit: Towards extremely low-bit large language models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qinghao and Liu, Zhiyuan and Sun, Maosong and Li, Peng},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}

@article{Wang:2023,
  title={BitNet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@article{Huang:2024,
  title={BiLLM: Pushing the limit of post-training quantization for LLMs},
  author={Huang, Wei and Liu, Yangdong and Qin, Haotong and Li, Ying and Zhang, Shiming and Liu, Xianglong and Magno, Michele and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2402.04291},
  year={2024}
}